<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Video Based Human Motion Prediction | Aadi Kothari </title> <meta name="author" content="Aadi Kothari"> <meta name="description" content="A simple study to setup a pipeline for video based human motion prediction."> <meta name="keywords" content="aadi kothari, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot_favicon.png?eb0175041d08c01fbda143012344a770"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aadi0902.github.io/blog/2022/video-based-human-motion-prediction/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Aadi</span> Kothari </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Video Based Human Motion Prediction</h1> <p class="post-meta"> Created on December 01, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/motion-prediction"> <i class="fa-solid fa-hashtag fa-sm"></i> motion_prediction</a>   <a href="/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> robotics</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer_vision</a>   ·   <a href="/blog/category/robotics"> <i class="fa-solid fa-tag fa-sm"></i> robotics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Human motion prediction is the process of anticipating the motion of humans based on observed data. This can vary from predicting the direction and speed of a person to characteristics like joint position prediction. It has a wide range of applications ranging from robotics – specifically human robot collaboration and collision avoidance as well as applications in sports and human computer interaction.</p> <p>Deep learning has proven to be a powerful tool for predicting human motion and in this blog we will explore a step by step approach towards predicting future human poses in real time using a single camera stream. We will discuss some of the challenges in making this pipeline work in real time as well as some of the shortcomings of the deep learning methods implemented.</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/pipeline_img.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <h2 id="human-detection---detection-transformer-detr">Human detection - DEtection TRansformer (DETR)</h2> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/bbox_inference.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the human detection layer of our pipeline, our goal is to generate bounding boxes around detected humans in a given frame. Due to the realtime nature of our application, equal consdieration is given to <strong>inference time</strong> and <strong><a href="https://www.v7labs.com/blog/mean-average-precision#:~:text=let's%20dive%20in!-,What%20is%20Mean%20Average%20Precision%20(mAP)%3F,values%20from%200%20to%201." rel="external nofollow noopener" target="_blank">mean average precision (mAP)</a></strong></p> <p>Comparison on Coco 2017 validation set:</p> <table> <thead> <tr> <th>Model</th> <th style="text-align: center">Inference FPS</th> <th style="text-align: center">AP</th> <th style="text-align: center">AP 50</th> <th style="text-align: center">AP 75</th> <th style="text-align: center">AP S</th> <th style="text-align: center">AP M</th> <th style="text-align: center">AP L</th> </tr> </thead> <tbody> <tr> <td>Faster R-CNN+FPN</td> <td style="text-align: center">26</td> <td style="text-align: center">42.0</td> <td style="text-align: center">62.1</td> <td style="text-align: center">45.5</td> <td style="text-align: center">26.6</td> <td style="text-align: center">45.4</td> <td style="text-align: center">53.4</td> </tr> <tr> <td>Deformable DETR</td> <td style="text-align: center">19</td> <td style="text-align: center"><strong>43.8</strong></td> <td style="text-align: center"><strong>62.6</strong></td> <td style="text-align: center"><strong>47.7</strong></td> <td style="text-align: center">26.4</td> <td style="text-align: center"><strong>47.1</strong></td> <td style="text-align: center">58.0</td> </tr> <tr> <td><strong>DETR</strong></td> <td style="text-align: center"><strong>28</strong></td> <td style="text-align: center">42.0</td> <td style="text-align: center">62.4</td> <td style="text-align: center">44.2</td> <td style="text-align: center">20.5</td> <td style="text-align: center">45.8</td> <td style="text-align: center"><strong>61.1</strong></td> </tr> </tbody> </table> <p>Based on the above data, the models have comparable Mean Average Precision (mAP) but since DETR outperforms on inference speed, we choose the same for our implementation.</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/detr_model.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As shown in the above image, the main architecture of DETR can be broken down into a pipeline of three main components:</p> <h4 id="1-cnn-backbone">1. CNN backbone</h4> <p>The conventional CNN backbone is used to learn a 2D representation of an input image.</p> <h4 id="2-encoder-decoder-transformer">2. Encoder-decoder transformer</h4> <p>The flattened representation from the CNN is combined with positional encoding and feeded into a transformer encoder. The transformer decoder then takes a fixed number of learned encodings while attending to the encoder output.</p> <h4 id="3-feed-forward-network-ffn">3. Feed Forward Network (FFN)</h4> <p>The output of the decoder is passed to a shared feed forward network (FFN) that either predicts a detection or “no object” class.</p> <hr> <h2 id="2d-pose-detection---video-pose-estimation-via-neural-architectural-search-vipnas">2D pose detection - Video Pose Estimation via Neural Architectural Search (ViPNAS)</h2> <p><br></p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/pose_inference.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Human pose estimation is the process of detecting the pose of a person in a given image. For a given skeletal model, the goal is to detect <em>keypoints</em> (joints) of a human in a given image frame, followed by joining respective keypoints to generate a skeletal representation of the person. Again, we equally consider inference speed and accuracy tradeoff to select the best model for 2D pose detection.</p> <table> <thead> <tr> <th>Model</th> <th style="text-align: center">Inference FPS</th> <th style="text-align: center">AP</th> </tr> </thead> <tbody> <tr> <td>ResNet</td> <td style="text-align: center">29</td> <td style="text-align: center">0.72</td> </tr> <tr> <td>ShuffleNet</td> <td style="text-align: center"><strong>63</strong></td> <td style="text-align: center">0.6</td> </tr> <tr> <td>HRNet</td> <td style="text-align: center">22</td> <td style="text-align: center"><strong>0.75</strong></td> </tr> <tr> <td><strong>ViPNAS + MobileNet</strong></td> <td style="text-align: center">54</td> <td style="text-align: center">0.7</td> </tr> </tbody> </table> <p><br></p> <p>For fast online video pose estimation, while achieving better trade off between accuracy and efficiency, we select ViPNAS for online pose estimation due to efficient pose estimation. The key to this architecture’s efficiency is allocation of different computational resources to different frames. <br><br></p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/vipnas_model_architecture.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> As shown above, the architecture can be decomposed into the following steps:</p> <ol> <li>Select a set of T + 1 frames in a video</li> <li>Out of the selected frames, select 1st frame as the key frame</li> <li>Apply spatial video pose estimation network (S-ViPNet) on the same to localize human poses.</li> <li>Use heat maps to encode joint locations as Gaussian peaks.</li> <li>For the non-key frames using temporal video pose estimation network (T-ViPNets):</li> <li>Some CNN layers are used for feature extraction.</li> <li>Fuse features of current frame with heat maps of last frame</li> <li>Pass fused features through remaining CNN layers to obtain heat maps.</li> </ol> <p>The main analogy here is that poses in adjacent video frames are temporally correlated and thus light weight models like T-VipNets can reasonably estimate joint locations with guidance from previous frames.</p> <hr> <h2 id="2d-to-3d-pose-lifting---videopose3d">2D to 3D pose lifting - VideoPose3D</h2> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/2d_to_3d_inference.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The goal of this layer is to infer the 3D poses given the 2D pose estimates from ViPNAS. VideoPose3D is a full convolutional architecture with residual connections and temporal convolutions as show below</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/video_pose3d_arch.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The reason for picking temporal convolutions over RNNs is:</p> <ol> <li>Convolutional architecture offers precise control over temporal receptive fields, which the authors of the paper found important for 3D pose estimation.</li> <li>Convolutional models enable parallelization over batch and time dimension and also do not suffer from vanishing and exploding gradients.</li> </ol> <p>To improve settings where labeled 3D ground-truth pose data is limited,a semi-supervised learning method is used as shown:</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/video_pose3d_sem_supervised.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Essentially an unlabeled video with off the shelf 2D keypoint detector is used for a supervised loss function with back propagation loss term, and then the problem is setup as an auto encoding problem:</p> <ol> <li>Encoder: 2D joint coordinates -&gt; 3D pose estimation</li> <li>Decoder 3D pose -&gt; 2D joint coordinates</li> </ol> <p>Lastly, since we require global position as well, 3D trajectory of the person is regressed by a very similar model compared to the pose model. ___</p> <h2 id="3d-pose-prediction">3D pose prediction</h2> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/pose_prediction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The goal of 3D pose prediction is given a past horizon of 3D joint poses, predict future joint poses over a given time horizon. For this task, we use the architecture called History Repeats Itself (HisRepItself) which tackles human motion prediction via motion attention.</p> <p>For the purposes of this approach, we purely look at past frames over a certain horizon to predict future human poses over another horizon. The underlying hypothesis is since humans tend to repeat motion across long time periods, sub sequences in motion history can be discovered via motion attention.</p> <p>This motion attention is then feeded in to the prediction model which consists of a Graph Convolutional Network (GCN) to capture the spatial relationship of the human skeletal joints.</p> <p>The visualization of the architecture is shown below:</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/HisRepItself_architecture.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The past poses are shown as blue and red skeletons and predicted ones and green and purple. For a given time frame (M consecutive poses), the Discrete Cosine Transforms are weighed using the computed attention score, and the weighted sum are combined with DCT coefficients of last sub-sequence in the prediction layer to future predictions of human poses. ___</p> <h2 id="inference-results">Inference results</h2> <p>Since the eventual goal of this approach is to leverage accurate human joint pose predictions in a human robot collaborative setting, we need to give equal importance to accuracy and inference speed. While we leave accuracy evaluations to test datasets, we analyze inference speeds for evaluating the application of this approach in a real time setting.</p> <p>The given pipeline structured is implemented on a live video stream coming from a webcam and inference speeds for each subtask are calculated as shown below:</p> <table> <thead> <tr> <th> </th> <th style="text-align: center">DETR</th> <th style="text-align: center">ViPNAS</th> <th style="text-align: center">VideoPose3D</th> <th style="text-align: center">Motion attention</th> <th style="text-align: center">Overall fps</th> </tr> </thead> <tbody> <tr> <td>Inference speed (fps)</td> <td style="text-align: center">28</td> <td style="text-align: center">54</td> <td style="text-align: center">280</td> <td style="text-align: center">31</td> <td style="text-align: center"><strong>11</strong></td> </tr> </tbody> </table> <p>Fast inference speeds are crucial in a realtime human robot collaboration task due to which recently we have been seeing a rise in well engineered models like ViPNAS where heavy models are tied together with light weight models in a key frame - non-key frame approach in order make these models fast while not compromising on accuracy.</p> <hr> <h2 id="challenges-and-key-takeaways">Challenges and key takeaways:</h2> <p>The key takeaway from this pipeline based approach for human motion prediction is interpretability of the sub tasks. By breaking down the goal into individual deep learning based models instead of learning end to end, we are able to identify and debug errors individually as well as the integration of domain knowledge in every sub task becomes easier.</p> <p>Also due to the modular approach, it easy to replace certain parts of the model depending on the task at hand and retrain certain parts instead of going through the pain of retraining the entire model. Analyzing performance of individual components including attributes like inference speed makes it easier to highlight pain points of an approach and select and fine tune models accordingly, thus helping scale the architecture.</p> <p>That being said, there are plenty of challenges that still need to be addressed in human motion prediction including some arising from the pipeline based approach:</p> <ol> <li>Robustness to missing joint positions and noisy data: It was noted that when using keypoint detections, often times there were missing joints due to the human being in different orientation or partially outside the frame. Also, when 2D to 3D pose lifting, often times the joint data would be noisy which led to overall bad human pose predictions. A few ways to make this approach more robust is by performing some kind of data imputation on missing joints data, training the predictor with augmented data that has noise as well as missing joints, thus making sure such cases are not our of distribution.</li> <li>Inference speed: As mentioned earlier, inference speed still remains a challenge in the application of this setup in a realtime collaborative setting. Around 11fps of overall inference speed is still not fast enough for high speed real time tasks and hence inference speed still remains a challenge.</li> <li>Error and uncertainty propagation: We highlighted that there was some noise and uncertainty associated with inferring the human joint pose data which also included errors where there was false detection, and thus throwing the predictor off in the downstream task of prediction. Also how uncertainties and errors from one block of the pipeline propagate to the other still remains an open question.</li> <li>Incorporating context based information: As we noted in the prediction part, we solely rely on past human poses to predict the future ones. In reality, this is not the case since a lot of times we use scene based contextual information to drive one’s motion.</li> <li>Incorporating joint constraints and fusing physics based models: Lastly, since this is a well studied dynamics problem, one possibility to improve this approach is to fuse physics based models as well as employ hard joint constraints in our model.</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Thus we study a pipeline based approach towards human motion prediction from a stream of camera frames in the form of a video. We highlight the pros and cons of this approach as well as our reasoning for picking specific models for each individual subtask. We also highlight potential research avenues as well discuss what particular challenges we came across when experimenting with this approach in an actual setup.</p> <h3 id="references">References</h3> <p>[1] Zhu Xizhou, Su Weijie, Lu Lewei, Li Bin, Wang Xiaogang, and Dai Jifeng. 2020. Deformable DETR: Deformable transformers for end-to-end object detection. arXiv:2010.04159. Retrieved from https://arxiv.org/abs/2010.04159.</p> <p>[2] Xu L.-M., Guan Y.-D., Jin S., Liu W.-T., Qian C., Luo P., Ouyang W.-L., and Wang X.-G.. 2021. ViPNAS: Efficient video pose estimation via neural architecture search. In CVPR.</p> <p>[3] Pavllo, Dario, et al. “3d human pose estimation in video with temporal convolutions and semi-supervised training.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p> <p>[4] Mao, Wei, Miaomiao Liu, and Mathieu Salzmann. “History repeats itself: Human motion prediction via motion attention.” European Conference on Computer Vision. Springer, Cham, 2020.</p> <p>[5] Sengupta, Arindam, et al. “mm-Pose: Real-time human skeletal posture estimation using mmWave radars and CNNs.” IEEE Sensors Journal 20.17 (2020): 10032-10044.</p> <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1HSZE1Y7M"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-V1HSZE1Y7M");</script> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Aadi Kothari. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-video-based-human-motion-prediction",title:"Video Based Human Motion Prediction",description:"A simple study to setup a pipeline for video based human motion prediction.",section:"Posts",handler:()=>{window.location.href="/blog/2022/video-based-human-motion-prediction/"}},{id:"news-graduated-from-mit-with-a-master-39-s-in-mechanical-engineering",title:"Graduated from MIT  with a master&#39;s in Mechanical Engineering.",description:"",section:"News"},{id:"news-joined-bastian-solutions-a-toyota-advanced-logistics-company-as-a-robotics-research-and-development-engineer",title:"Joined Bastian Solutions (a Toyota advanced logistics company) as a robotics research and...",description:"",section:"News"},{id:"projects-autonomous-excavator",title:"Autonomous Excavator",description:"autonomous excavator as part of introduction to robotics course project.",section:"Projects",handler:()=>{window.location.href="/projects/autonomous_excavator/"}},{id:"projects-robotics-r-amp-d-engineer-ii",title:"Robotics R&amp;D Engineer II",description:"robotics engineer at Bastian Solutions - a toyota advanced logistics company",section:"Projects",handler:()=>{window.location.href="/projects/bastian_robotics_r_n_d/"}},{id:"projects-engineering-make-a-thons",title:"Engineering Make-a-thons",description:"weekend long competitions for creative engineering solutions.",section:"Projects",handler:()=>{window.location.href="/projects/comets_create/"}},{id:"projects-controls-optimization-and-networks-lab",title:"Controls Optimization and Networks Lab",description:"my work at Controls Optimization and Networks Lab at UT Dallas",section:"Projects",handler:()=>{window.location.href="/projects/con_lab/"}},{id:"projects-design-build-fly",title:"Design Build Fly",description:"details of my work for Design Build Fly competition 2018-2020",section:"Projects",handler:()=>{window.location.href="/projects/design_build_fly/"}},{id:"projects-drawing-robot",title:"Drawing Robot",description:"drawing robot as part of engineering projects in community service.",section:"Projects",handler:()=>{window.location.href="/projects/drawing_robot/"}},{id:"projects-human-robot-collaboration",title:"Human Robot Collaboration",description:"my graduate research thesis at MIT.",section:"Projects",handler:()=>{window.location.href="/projects/human_robot_collaboration/"}},{id:"projects-mit-driverless",title:"MIT Driverless",description:"autonomous vehicles racing as part of Indy Autonomous Challenge.",section:"Projects",handler:()=>{window.location.href="/projects/mit_driverless/"}},{id:"projects-robotic-submarine",title:"Robotic submarine",description:"details of robotic submarine (robosub).",section:"Projects",handler:()=>{window.location.href="/projects/robosub/"}},{id:"projects-robotic-emulator",title:"Robotic Emulator",description:"robotic emulator for offshore wind turbine testing.",section:"Projects",handler:()=>{window.location.href="/projects/robotic_emulator/"}},{id:"projects-robotics-engineer-internship",title:"Robotics Engineer Internship",description:"internship at Pickle Robot Company 2022",section:"Projects",handler:()=>{window.location.href="/projects/robotics_engineer_internship/"}},{id:"projects-wind-energy-research",title:"Wind energy research",description:"my work at Griffith Research Lab at UT Dallas.",section:"Projects",handler:()=>{window.location.href="/projects/wind_turbine_research/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%61%64%69%61%61%64%69%31%32%33%34%35@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0001-7339-6120","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2LjkEg8AAAAJ&hl","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Aadi0902","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/aadikothari","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>