<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://aadi0902.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aadi0902.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-15T05:22:21+00:00</updated><id>https://aadi0902.github.io/feed.xml</id><title type="html">blank</title><subtitle>Aadi Kothari&apos;s website. Includes blogs, CV, projects, repositories, etc. </subtitle><entry><title type="html">Video Based Human Motion Prediction</title><link href="https://aadi0902.github.io/blog/2022/video-based-human-motion-prediction/" rel="alternate" type="text/html" title="Video Based Human Motion Prediction"/><published>2022-12-01T09:00:00+00:00</published><updated>2022-12-01T09:00:00+00:00</updated><id>https://aadi0902.github.io/blog/2022/video-based-human-motion-prediction</id><content type="html" xml:base="https://aadi0902.github.io/blog/2022/video-based-human-motion-prediction/"><![CDATA[<p>Human motion prediction is the process of anticipating the motion of humans based on observed data. This can vary from predicting the direction and speed of a person to characteristics like joint position prediction. It has a wide range of applications ranging from robotics – specifically human robot collaboration and collision avoidance as well as applications in sports and human computer interaction.</p> <p>Deep learning has proven to be a powerful tool for predicting human motion and in this blog we will explore a step by step approach towards predicting future human poses in real time using a single camera stream. We will discuss some of the challenges in making this pipeline work in real time as well as some of the shortcomings of the deep learning methods implemented.</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/pipeline_img.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="human-detection---detection-transformer-detr">Human detection - DEtection TRansformer (DETR)</h2> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/bbox_inference.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In the human detection layer of our pipeline, our goal is to generate bounding boxes around detected humans in a given frame. Due to the realtime nature of our application, equal consdieration is given to <strong>inference time</strong> and <strong><a href="https://www.v7labs.com/blog/mean-average-precision#:~:text=let's%20dive%20in!-,What%20is%20Mean%20Average%20Precision%20(mAP)%3F,values%20from%200%20to%201.">mean average precision (mAP)</a></strong></p> <p>Comparison on Coco 2017 validation set:</p> <table> <thead> <tr> <th>Model</th> <th style="text-align: center">Inference FPS</th> <th style="text-align: center">AP</th> <th style="text-align: center">AP 50</th> <th style="text-align: center">AP 75</th> <th style="text-align: center">AP S</th> <th style="text-align: center">AP M</th> <th style="text-align: center">AP L</th> </tr> </thead> <tbody> <tr> <td>Faster R-CNN+FPN</td> <td style="text-align: center">26</td> <td style="text-align: center">42.0</td> <td style="text-align: center">62.1</td> <td style="text-align: center">45.5</td> <td style="text-align: center">26.6</td> <td style="text-align: center">45.4</td> <td style="text-align: center">53.4</td> </tr> <tr> <td>Deformable DETR</td> <td style="text-align: center">19</td> <td style="text-align: center"><strong>43.8</strong></td> <td style="text-align: center"><strong>62.6</strong></td> <td style="text-align: center"><strong>47.7</strong></td> <td style="text-align: center">26.4</td> <td style="text-align: center"><strong>47.1</strong></td> <td style="text-align: center">58.0</td> </tr> <tr> <td><strong>DETR</strong></td> <td style="text-align: center"><strong>28</strong></td> <td style="text-align: center">42.0</td> <td style="text-align: center">62.4</td> <td style="text-align: center">44.2</td> <td style="text-align: center">20.5</td> <td style="text-align: center">45.8</td> <td style="text-align: center"><strong>61.1</strong></td> </tr> </tbody> </table> <p>Based on the above data, the models have comparable Mean Average Precision (mAP) but since DETR outperforms on inference speed, we choose the same for our implementation.</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/detr_model.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As shown in the above image, the main architecture of DETR can be broken down into a pipeline of three main components:</p> <h4 id="1-cnn-backbone">1. CNN backbone</h4> <p>The conventional CNN backbone is used to learn a 2D representation of an input image.</p> <h4 id="2-encoder-decoder-transformer">2. Encoder-decoder transformer</h4> <p>The flattened representation from the CNN is combined with positional encoding and feeded into a transformer encoder. The transformer decoder then takes a fixed number of learned encodings while attending to the encoder output.</p> <h4 id="3-feed-forward-network-ffn">3. Feed Forward Network (FFN)</h4> <p>The output of the decoder is passed to a shared feed forward network (FFN) that either predicts a detection or “no object” class.</p> <hr/> <h2 id="2d-pose-detection---video-pose-estimation-via-neural-architectural-search-vipnas">2D pose detection - Video Pose Estimation via Neural Architectural Search (ViPNAS)</h2> <p><br/></p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/pose_inference.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Human pose estimation is the process of detecting the pose of a person in a given image. For a given skeletal model, the goal is to detect <em>keypoints</em> (joints) of a human in a given image frame, followed by joining respective keypoints to generate a skeletal representation of the person. Again, we equally consider inference speed and accuracy tradeoff to select the best model for 2D pose detection.</p> <table> <thead> <tr> <th>Model</th> <th style="text-align: center">Inference FPS</th> <th style="text-align: center">AP</th> </tr> </thead> <tbody> <tr> <td>ResNet</td> <td style="text-align: center">29</td> <td style="text-align: center">0.72</td> </tr> <tr> <td>ShuffleNet</td> <td style="text-align: center"><strong>63</strong></td> <td style="text-align: center">0.6</td> </tr> <tr> <td>HRNet</td> <td style="text-align: center">22</td> <td style="text-align: center"><strong>0.75</strong></td> </tr> <tr> <td><strong>ViPNAS + MobileNet</strong></td> <td style="text-align: center">54</td> <td style="text-align: center">0.7</td> </tr> </tbody> </table> <p><br/></p> <p>For fast online video pose estimation, while achieving better trade off between accuracy and efficiency, we select ViPNAS for online pose estimation due to efficient pose estimation. The key to this architecture’s efficiency is allocation of different computational resources to different frames. <br/><br/></p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/vipnas_model_architecture.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><br/> As shown above, the architecture can be decomposed into the following steps:</p> <ol> <li>Select a set of T + 1 frames in a video</li> <li>Out of the selected frames, select 1st frame as the key frame</li> <li>Apply spatial video pose estimation network (S-ViPNet) on the same to localize human poses.</li> <li>Use heat maps to encode joint locations as Gaussian peaks.</li> <li>For the non-key frames using temporal video pose estimation network (T-ViPNets):</li> <li>Some CNN layers are used for feature extraction.</li> <li>Fuse features of current frame with heat maps of last frame</li> <li>Pass fused features through remaining CNN layers to obtain heat maps.</li> </ol> <p>The main analogy here is that poses in adjacent video frames are temporally correlated and thus light weight models like T-VipNets can reasonably estimate joint locations with guidance from previous frames.</p> <hr/> <h2 id="2d-to-3d-pose-lifting---videopose3d">2D to 3D pose lifting - VideoPose3D</h2> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/2d_to_3d_inference.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The goal of this layer is to infer the 3D poses given the 2D pose estimates from ViPNAS. VideoPose3D is a full convolutional architecture with residual connections and temporal convolutions as show below</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/video_pose3d_arch.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The reason for picking temporal convolutions over RNNs is:</p> <ol> <li>Convolutional architecture offers precise control over temporal receptive fields, which the authors of the paper found important for 3D pose estimation.</li> <li>Convolutional models enable parallelization over batch and time dimension and also do not suffer from vanishing and exploding gradients.</li> </ol> <p>To improve settings where labeled 3D ground-truth pose data is limited,a semi-supervised learning method is used as shown:</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/video_pose3d_sem_supervised.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Essentially an unlabeled video with off the shelf 2D keypoint detector is used for a supervised loss function with back propagation loss term, and then the problem is setup as an auto encoding problem:</p> <ol> <li>Encoder: 2D joint coordinates -&gt; 3D pose estimation</li> <li>Decoder 3D pose -&gt; 2D joint coordinates</li> </ol> <p>Lastly, since we require global position as well, 3D trajectory of the person is regressed by a very similar model compared to the pose model. ___</p> <h2 id="3d-pose-prediction">3D pose prediction</h2> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/pose_prediction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The goal of 3D pose prediction is given a past horizon of 3D joint poses, predict future joint poses over a given time horizon. For this task, we use the architecture called History Repeats Itself (HisRepItself) which tackles human motion prediction via motion attention.</p> <p>For the purposes of this approach, we purely look at past frames over a certain horizon to predict future human poses over another horizon. The underlying hypothesis is since humans tend to repeat motion across long time periods, sub sequences in motion history can be discovered via motion attention.</p> <p>This motion attention is then feeded in to the prediction model which consists of a Graph Convolutional Network (GCN) to capture the spatial relationship of the human skeletal joints.</p> <p>The visualization of the architecture is shown below:</p> <figure> <picture> <img src="/assets/posts/video_based_human_motion_prediction/HisRepItself_architecture.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The past poses are shown as blue and red skeletons and predicted ones and green and purple. For a given time frame (M consecutive poses), the Discrete Cosine Transforms are weighed using the computed attention score, and the weighted sum are combined with DCT coefficients of last sub-sequence in the prediction layer to future predictions of human poses. ___</p> <h2 id="inference-results">Inference results</h2> <p>Since the eventual goal of this approach is to leverage accurate human joint pose predictions in a human robot collaborative setting, we need to give equal importance to accuracy and inference speed. While we leave accuracy evaluations to test datasets, we analyze inference speeds for evaluating the application of this approach in a real time setting.</p> <p>The given pipeline structured is implemented on a live video stream coming from a webcam and inference speeds for each subtask are calculated as shown below:</p> <table> <thead> <tr> <th> </th> <th style="text-align: center">DETR</th> <th style="text-align: center">ViPNAS</th> <th style="text-align: center">VideoPose3D</th> <th style="text-align: center">Motion attention</th> <th style="text-align: center">Overall fps</th> </tr> </thead> <tbody> <tr> <td>Inference speed (fps)</td> <td style="text-align: center">28</td> <td style="text-align: center">54</td> <td style="text-align: center">280</td> <td style="text-align: center">31</td> <td style="text-align: center"><strong>11</strong></td> </tr> </tbody> </table> <p>Fast inference speeds are crucial in a realtime human robot collaboration task due to which recently we have been seeing a rise in well engineered models like ViPNAS where heavy models are tied together with light weight models in a key frame - non-key frame approach in order make these models fast while not compromising on accuracy.</p> <hr/> <h2 id="challenges-and-key-takeaways">Challenges and key takeaways:</h2> <p>The key takeaway from this pipeline based approach for human motion prediction is interpretability of the sub tasks. By breaking down the goal into individual deep learning based models instead of learning end to end, we are able to identify and debug errors individually as well as the integration of domain knowledge in every sub task becomes easier.</p> <p>Also due to the modular approach, it easy to replace certain parts of the model depending on the task at hand and retrain certain parts instead of going through the pain of retraining the entire model. Analyzing performance of individual components including attributes like inference speed makes it easier to highlight pain points of an approach and select and fine tune models accordingly, thus helping scale the architecture.</p> <p>That being said, there are plenty of challenges that still need to be addressed in human motion prediction including some arising from the pipeline based approach:</p> <ol> <li>Robustness to missing joint positions and noisy data: It was noted that when using keypoint detections, often times there were missing joints due to the human being in different orientation or partially outside the frame. Also, when 2D to 3D pose lifting, often times the joint data would be noisy which led to overall bad human pose predictions. A few ways to make this approach more robust is by performing some kind of data imputation on missing joints data, training the predictor with augmented data that has noise as well as missing joints, thus making sure such cases are not our of distribution.</li> <li>Inference speed: As mentioned earlier, inference speed still remains a challenge in the application of this setup in a realtime collaborative setting. Around 11fps of overall inference speed is still not fast enough for high speed real time tasks and hence inference speed still remains a challenge.</li> <li>Error and uncertainty propagation: We highlighted that there was some noise and uncertainty associated with inferring the human joint pose data which also included errors where there was false detection, and thus throwing the predictor off in the downstream task of prediction. Also how uncertainties and errors from one block of the pipeline propagate to the other still remains an open question.</li> <li>Incorporating context based information: As we noted in the prediction part, we solely rely on past human poses to predict the future ones. In reality, this is not the case since a lot of times we use scene based contextual information to drive one’s motion.</li> <li>Incorporating joint constraints and fusing physics based models: Lastly, since this is a well studied dynamics problem, one possibility to improve this approach is to fuse physics based models as well as employ hard joint constraints in our model.</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Thus we study a pipeline based approach towards human motion prediction from a stream of camera frames in the form of a video. We highlight the pros and cons of this approach as well as our reasoning for picking specific models for each individual subtask. We also highlight potential research avenues as well discuss what particular challenges we came across when experimenting with this approach in an actual setup.</p> <h3 id="references">References</h3> <p>[1] Zhu Xizhou, Su Weijie, Lu Lewei, Li Bin, Wang Xiaogang, and Dai Jifeng. 2020. Deformable DETR: Deformable transformers for end-to-end object detection. arXiv:2010.04159. Retrieved from https://arxiv.org/abs/2010.04159.</p> <p>[2] Xu L.-M., Guan Y.-D., Jin S., Liu W.-T., Qian C., Luo P., Ouyang W.-L., and Wang X.-G.. 2021. ViPNAS: Efficient video pose estimation via neural architecture search. In CVPR.</p> <p>[3] Pavllo, Dario, et al. “3d human pose estimation in video with temporal convolutions and semi-supervised training.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p> <p>[4] Mao, Wei, Miaomiao Liu, and Mathieu Salzmann. “History repeats itself: Human motion prediction via motion attention.” European Conference on Computer Vision. Springer, Cham, 2020.</p> <p>[5] Sengupta, Arindam, et al. “mm-Pose: Real-time human skeletal posture estimation using mmWave radars and CNNs.” IEEE Sensors Journal 20.17 (2020): 10032-10044.</p> <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V1HSZE1Y7M"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-V1HSZE1Y7M");</script>]]></content><author><name></name></author><category term="robotics"/><category term="motion_prediction"/><category term="robotics"/><category term="computer_vision"/><summary type="html"><![CDATA[A simple study to setup a pipeline for video based human motion prediction.]]></summary></entry></feed>